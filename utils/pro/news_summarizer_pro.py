import gradio as gr
import feedparser
import requests
import subprocess
from duckduckgo_search import DDGS
from newspaper import Article
from bs4 import BeautifulSoup
from textbox_with_stt_final_pro import TextboxWithSTTPro
from llm_interface_pro import LLMInterfacePro

class NewsSummarizer:
    def __init__(self):
        self.titles_links_global = []
        self.RSS_FEEDS = {
            "BBC": "http://feeds.bbci.co.uk/news/rss.xml",
            "Reuters": "http://feeds.reuters.com/reuters/topNews",
            "CNN": "http://rss.cnn.com/rss/edition.rss",
            "–£–ù–ò–ê–ù": "https://rss.unian.net/site/news_ukr.rss"
        }

    # ---------- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ----------
    def search_duckduckgo(self, query, num=10):
        results = []
        with DDGS() as ddgs:
            for r in ddgs.news(query, max_results=num):
                results.append({
                    "title": r.get("title"),
                    "link": r.get("url"),
                    "source": r.get("source"),
                    "date": r.get("date")
                })
        return results

    def search_newsapi(self, query, num=10, api_key="YOUR_API_KEY"):
        url = f"https://newsapi.org/v2/everything?q={query}&pageSize={num}&apiKey={api_key}"
        r = requests.get(url).json()
        results = []
        for article in r.get("articles", []):
            results.append({
                "title": article.get("title"),
                "link": article.get("url"),
                "source": article.get("source", {}).get("name"),
                "date": article.get("publishedAt")
            })
        return results

    def search_rss(self, num=10):
        results = []
        for name, url in self.RSS_FEEDS.items():
            feed = feedparser.parse(url)
            for entry in feed.entries[:num//len(self.RSS_FEEDS)]:
                results.append({
                    "title": entry.get("title"),
                    "link": entry.get("link"),
                    "source": name,
                    "date": entry.get("published", "")
                })
        return results

    def search_news(self, query, method):
        if method == "DuckDuckGo":
            return self.search_duckduckgo(query)
        elif method == "NewsAPI":
            return self.search_newsapi(query)
        elif method == "RSS":
            return self.search_rss()
        else:
            return [{"title": "–û—à–∏–±–∫–∞: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥", "link": "", "source": "", "date": ""}]

    # ---------- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
    def format_results(self, results):
        if not results:
            self.titles_links_global = []
            return "<b>–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.</b>", []
        html = "<ol>"
        self.titles_links_global = []
        news_titles = []
        for r in results:
            title = r['title'] or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            source = r['source'] or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"
            date = r['date'] or ""
            link = r['link'] or "#"
            html += f"<li><a href='{link}' target='_blank'>{title}</a> <i>({source}, {date})</i></li>"
            self.titles_links_global.append((title, link))
            news_titles.append(title)
        html += "</ol>"
        return html, news_titles

    # ---------- –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Ollama ----------
    def get_ollama_models(self):
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            models = []
            for line in result.stdout.splitlines()[1:]:
                parts = line.split()
                if parts:
                    models.append(parts[0])
            return models if models else ["–ù–µ—Ç –º–æ–¥–µ–ª–µ–π"]
        except Exception as e:
            return [f"–û—à–∏–±–∫–∞: {e}"]

    # ---------- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ ----------
    def extract_article_text(self, url):
        try:
            article = Article(
                url,
                language='ru',
                browser_user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                                    "Chrome/120.0.0.0 Safari/537.36"))
            article.download()
            article.parse()
            if article.text and len(article.text.split()) > 50:
                return article.text
        except Exception:
            pass

        try:
            headers = {"User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                                      "Chrome/120.0.0.0 Safari/537.36")}
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")

            paragraphs = [p.get_text() for p in soup.find_all("p")]
            text = "\n".join(paragraphs).strip()

            if text and len(text.split()) > 30:
                return text
            else:
                return "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (—Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞)"
        except Exception as e:
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏: {e}"

    # ---------- –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ ----------
    def summarize_news(self, news_text, prompt, llm_model):
        try:
            full_prompt = f"{prompt}\n\n{news_text}"
            result = subprocess.run(
                ["ollama", "run", llm_model],
                input=full_prompt,
                text=True,
                capture_output=True
            )
            return result.stdout.strip() if result.stdout else "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {e}"

    # ---------- UI ----------
    def create_interface(self):
        with gr.Blocks() as interface:
            gr.Markdown("## üì∞ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π")

            with gr.Row():
                topic = TextboxWithSTTPro(label="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É", value="–í–æ–π–Ω–∞ –≤ –£–∫—Ä–∞–∏–Ω–µ")
                method = gr.Radio(["DuckDuckGo", "NewsAPI", "RSS"], value="DuckDuckGo", label="–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞")

            search_btn = gr.Button("–ü–æ–∏—Å–∫")
            output = gr.HTML(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
             
            news_dropdown = gr.Dropdown(choices=[], label="–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è")

            extract_btn = gr.Button("–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏")
            extract_output = TextboxWithSTTPro(label="–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏", lines=10)

            llm_interface = LLMInterfacePro(
                title="–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å LLM (Ollama)",
                heading="–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å Ollama –¥–ª—è —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è",
                prompt_label="–ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏",
                input_label="–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏",
                input_placeholder="–ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è...",
                input_value="", 
                generate_button_text="–°–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—é–º–µ",
                output_label="–†–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏",  
                typical_prompts={"–ü–∏—Å–∞—Ç–µ–ª—å": "–¢—ã —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤—ã–π –ø–∏—Å–∞—Ç–µ–ª—å –∏ —Ä–∞—Å—Å–∫–∞–∑—á–∏–∫. –ü–∏—à–∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ, –∂–∏–≤–æ –∏ —Å –¥–µ—Ç–∞–ª—è–º–∏, —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏—Ç–∞—Ç–µ–ª—è. –°–æ—Å—Ç–∞–≤—å {param} –∏–∑ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."}, 
                prompt_params={"–ü–∏—Å–∞—Ç–µ–ª—å": ["–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ", "—Ä–∞—Å—Å–∫–∞–∑", "—ç—Å—Å–µ", "—Å—Ç–∞—Ç—å—è", "–ø–æ—ç–º–∞"]},
                default_prompt_index=0, default_param_index=0
                ) 
            
            gr.HTML("""<div style='height: 2px; background: linear-gradient(90deg, transparent, #666, transparent); margin: 40px 0;'></div>""")


            # ---------- –õ–æ–≥–∏–∫–∞ ----------
            def search_and_update_dropdown(query, method, dropdown):
                html, news_titles = self.format_results(self.search_news(query, method))
                return html, gr.update(choices=news_titles, value=news_titles[0] if news_titles else None)

            search_btn.click(fn=search_and_update_dropdown, 
                            inputs=[topic.textbox, method, news_dropdown], 
                            outputs=[output, news_dropdown])

            def extract_selected(news_choice):
                if not news_choice or not self.titles_links_global:
                    return "–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –∏ –º–æ–¥–µ–ª—å"
                link = None
                for title, lnk in self.titles_links_global:
                    if title == news_choice:
                        link = lnk
                        break
                if not link:
                    return "–°—Å—ã–ª–∫–∞ –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                news_text = self.extract_article_text(link)
                return news_text

            extract_btn.click(fn=extract_selected, 
                            inputs=[news_dropdown],
                            outputs=[extract_output.textbox]) 

            extract_output.textbox.change(fn=lambda x: x,  # –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ
                inputs=extract_output.textbox,
                outputs=llm_interface.input_box.textbox)
        return interface


if __name__ == "__main__":
    news_summarizer = NewsSummarizer()
    interface = news_summarizer.create_interface()
    interface.launch()
