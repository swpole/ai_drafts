# llm creates text
# inpput - text
# output - text

import gradio as gr
import ollama
import subprocess
import gc
from textbox_with_stt_final_pro import TextboxWithSTTPro

class LLMInterfacePro:
    def __init__(
        self,
        title: str = "Title",
        heading: str = "Heading",
        prompt_label: str = "–ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏",
        prompt_default: str = None,
        input_label: str = "Label input",
        input_placeholder: str = "–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ —Ç–µ–∫—Å—Ç ...",
        input_value: str = "Hi! How are you?",
        generate_button_text: str = "Label button",
        output_label: str = "Label output",
        typical_prompts: dict = {},
        prompt_params: dict = {},
        default_prompt_index: int = 0,
        default_param_index: int = 0,
    ):
        self.title = title
        self.heading = heading
        self.prompt_label = prompt_label
        self.input_label = input_label
        self.input_placeholder = input_placeholder
        self.input_value = input_value
        self.generate_button_text = generate_button_text
        self.output_label = output_label
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.loaded_models = {}

        # --- –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---
        self.typical_prompts = {
            "–≠–∫—Å–ø–µ—Ä—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç": "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç –∏ –ø–æ–º–æ—â–Ω–∏–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ {param}. –û—Ç–≤–µ—á–∞–π —á—ë—Ç–∫–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏ –¥–æ–±–∞–≤–ª—è–π –ø—Ä–∏–º–µ—Ä—ã. –û–±—ä—è—Å–Ω—è–π —à–∞–≥ –∑–∞ —à–∞–≥–æ–º, –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Å–ª–æ–∂–Ω–∞—è.",
            "–£—á–∏—Ç–µ–ª—å": "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —É—á–∏—Ç–µ–ª—å –ø–æ –ø—Ä–µ–¥–º–µ—Ç—É {param}. –û–±—ä—è—Å–Ω—è–π –º–∞—Ç–µ—Ä–∏–∞–ª –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –ø—Ä–∏–≤–æ–¥—è –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏.",
            "–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –≤ —Å—Ñ–µ—Ä–µ {param}. –°–Ω–∞—á–∞–ª–∞ –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –ø–æ –¥–µ–ª—É, –∑–∞—Ç–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ –æ–±—ä—è—Å–Ω–∏ –ª–æ–≥–∏–∫—É –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã.",
            "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –≤ –æ–±–ª–∞—Å—Ç–∏ {param}. –î–µ–ª–∞–π —Ä–∞–∑–±–æ—Ä –∑–∞–¥–∞—á–∏, –ø—Ä–∏–≤–æ–¥–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, —Å—Ä–∞–≤–Ω–∏–≤–∞–π —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –¥–µ–ª–∞–π –≤—ã–≤–æ–¥.",
            "–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫": "–ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∞ {param}, —Å–æ—Ö—Ä–∞–Ω–∏ —Å—Ç–∏–ª—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏, –ø–æ—è—Å–Ω–∏ –∏—Ö."
        }

        self.prompt_params = {
            "–≠–∫—Å–ø–µ—Ä—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç": ["–ò–ò", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–º–µ–¥–∏—Ü–∏–Ω–∞", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
            "–£—á–∏—Ç–µ–ª—å": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "—Ñ–∏–∑–∏–∫–∞", "–∏—Å—Ç–æ—Ä–∏—è", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞"],
            "–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç": ["–±–∏–∑–Ω–µ—Å", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—Å—Ç–∞—Ä—Ç–∞–ø—ã"],
            "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–Ω–∞—É–∫–∞", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–æ–±—â–µ—Å—Ç–≤–æ"],
            "–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫": ["–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "–Ω–µ–º–µ—Ü–∫–∏–π", "—è–ø–æ–Ω—Å–∫–∏–π", "–∫–∏—Ç–∞–π—Å–∫–∏–π"]
        }

        self.typical_prompts = {**typical_prompts, **self.typical_prompts}
        self.prompt_params = {**prompt_params, **self.prompt_params}

        # --- –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º ---
        keys = list(self.typical_prompts.keys())
        if default_prompt_index < 0 or default_prompt_index >= len(keys):
            default_prompt_index = 0
        self.default_prompt_key = keys[default_prompt_index]

        params = self.prompt_params.get(self.default_prompt_key, [""])
        if default_param_index < 0 or default_param_index >= len(params):
            default_param_index = 0
        self.default_param_value = params[default_param_index]

        # --- —Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ ---
        if prompt_default:
            self.prompt_default = prompt_default
        else:
            self.prompt_default = self.typical_prompts[self.default_prompt_key].replace("{param}", self.default_param_value)

        self.build_interface()

    def get_models(self):
        try:
            subprocess.run(["ollama", "list"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            models_info = ollama.list()
            return [m["model"] for m in models_info["models"]]
        except Exception as e:
            err = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e} –í–æ–∑–º–æ–∂–Ω–æ ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞."
            return [err]

    def load_model(self, model_name):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"""
        try:
            if model_name not in self.loaded_models:
                print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_name}")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å
                models_info = ollama.list()
                available_models = [m["model"] for m in models_info["models"]]
                
                if model_name not in available_models:
                    return f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(available_models)}"
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                ollama.chat(model=model_name, messages=[{"role": "user", "content": "ping"}])
                self.loaded_models[model_name] = True
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None  # –£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}"

    def unload_model(self, model_name):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏"""
        try:
            print(f"üóëÔ∏è –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_name}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama API –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            subprocess.run(["ollama", f"list"])
            subprocess.run(["ollama", f"stop", f"{model_name}"])
            
            del self.loaded_models[model_name]
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞
            gc.collect()
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")

    def unload_all_models(self):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True, check=True)
        output = result.stdout
        lines = output.strip().split('\n')
        running_models = []
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        for line in lines[1:]:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            if line.strip():  # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç (–∏–º—è –º–æ–¥–µ–ª–∏)
                model_name = line.split()[0]  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü - –∏–º—è –º–æ–¥–µ–ª–∏
                running_models.append(model_name)

        for model_name in running_models:
            self.unload_model(model_name)

    def generate(self, model_name, prompt, input_text):
        temperature = self.temperature_slider.value
        top_p = self.top_p_slider.value
        
        if not input_text or not input_text.strip():
            return "‚ö†Ô∏è " + self.input_label + " –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º."
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        load_error = self.load_model(model_name)
        if load_error:
            return load_error
        
        try:
            full_prompt = f"{prompt}\n\n{self.input_label}:\n{input_text}"
            response = ollama.chat(
                model=model_name,
                options={"temperature": temperature, "top_p": top_p},
                messages=[{"role": "user", "content": full_prompt}]
            )
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.unload_model(model_name)
            
            return response["message"]["content"]
            
        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Ç–∞–∫–∂–µ –ø—ã—Ç–∞–µ–º—Å—è –≤—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
            self.unload_model(model_name)
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"

    def on_model_change(self, model_name):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ - –≤—ã–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â—É—é –º–æ–¥–µ–ª—å"""
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏
        # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏
        pass

    def build_interface(self):
        models = self.get_models()

        # —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
        first_prompt_key = self.default_prompt_key
        first_param = self.default_param_value
        initial_prompt_text = self.prompt_default

        gr.Markdown(f"### üìù {self.heading}")

        with gr.Row():
            model_dropdown = gr.Dropdown(
                choices=models,
                value=models[0] if models else None,
                label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å"
            )

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é
        with gr.Accordion("üß† –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é", open=False):
            with gr.Row():
                load_model_btn = gr.Button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
                unload_model_btn = gr.Button("üóëÔ∏è –í—ã–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
                unload_all_btn = gr.Button("üßπ –í—ã–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏")
                memory_status_btn = gr.Button("üìä –°—Ç–∞—Ç—É—Å –ø–∞–º—è—Ç–∏")
            
            memory_status = gr.Textbox(
                label="–°—Ç–∞—Ç—É—Å –ø–∞–º—è—Ç–∏",
                interactive=False,
                lines=2
            )

        with gr.Accordion("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –º–æ–¥–µ–ª–∏", open=False):
            with gr.Row():
                typical_prompt_dd = gr.Dropdown(
                    choices=list(self.typical_prompts.keys()),
                    value=first_prompt_key,
                    label="–¢–∏–ø–∏—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç"
                )
                param_dd = gr.Dropdown(
                    choices=self.prompt_params.get(first_prompt_key, []),
                    value=first_param,
                    label="–ü–∞—Ä–∞–º–µ—Ç—Ä"
                )

            with gr.Row():
                self.temperature_slider = gr.Slider(
                    0.0, 1.0, value=0.7, step=0.1, label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞"
                )
                self.top_p_slider = gr.Slider(
                    0.0, 1.0, value=0.9, step=0.1, label="Top-p"
                )

            prompt_box = TextboxWithSTTPro(
                label=self.prompt_label,
                value=initial_prompt_text,
                lines=2
            )

        self.input_box = TextboxWithSTTPro(
            label=self.input_label,
            placeholder=self.input_placeholder,
            value=self.input_value,
            lines=5,
            max_lines=5
        )

        run_button = gr.Button(self.generate_button_text)

        self.output_box = TextboxWithSTTPro(
            label=self.output_label,
            lines=5,
            max_lines=5
        )

        # —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        def update_params(prompt_choice):
            return gr.update(
                choices=self.prompt_params.get(prompt_choice, []),
                value=self.prompt_params.get(prompt_choice, [""])[0]
            )

        def apply_prompt(prompt_choice, param_choice):
            if prompt_choice in self.typical_prompts:
                template = self.typical_prompts[prompt_choice]
                if "{param}" in template and param_choice:
                    return template.replace("{param}", param_choice)
                else:
                    if "{style}" in template and param_choice:
                        return template.replace("{style}", param_choice)
                    else:
                        return template
            return self.prompt_default

        # –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é
        def load_selected_model(model_name):
            result = self.load_model(model_name)
            return result if result else f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞"

        def unload_selected_model(model_name):
            self.unload_model(model_name)
            return f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞"

        def unload_all_models_wrapper():
            self.unload_all_models()
            return "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—ã–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏"

        def get_memory_status():
            loaded_count = len(self.loaded_models)
            status = f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {loaded_count}\n"
            status += f"üß† –ú–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç–∏: {', '.join(self.loaded_models.keys()) if self.loaded_models else '–Ω–µ—Ç'}"
            return status

        # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤
        typical_prompt_dd.change(
            fn=update_params,
            inputs=typical_prompt_dd,
            outputs=param_dd
        )
        typical_prompt_dd.change(
            fn=apply_prompt,
            inputs=[typical_prompt_dd, param_dd],
            outputs=prompt_box.textbox
        )
        param_dd.change(
            fn=apply_prompt,
            inputs=[typical_prompt_dd, param_dd],
            outputs=prompt_box.textbox
        )

        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é
        load_model_btn.click(
            fn=load_selected_model,
            inputs=model_dropdown,
            outputs=memory_status
        )
        
        unload_model_btn.click(
            fn=unload_selected_model,
            inputs=model_dropdown,
            outputs=memory_status
        )
        
        unload_all_btn.click(
            fn=unload_all_models_wrapper,
            outputs=memory_status
        )
        
        memory_status_btn.click(
            fn=get_memory_status,
            outputs=memory_status
        )

        run_button.click(
            fn=self.generate,
            inputs=[model_dropdown, prompt_box.textbox, self.input_box.textbox],
            outputs=self.output_box.textbox
        )

        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        model_dropdown.change(
            fn=self.on_model_change,
            inputs=model_dropdown
        )


if __name__ == "__main__":
    # –ø—Ä–∏–º–µ—Ä: –±–µ—Ä–µ–º 2-–π –ø—Ä–æ–º–ø—Ç ("–£—á–∏—Ç–µ–ª—å") –∏ 2-–π –ø–∞—Ä–∞–º–µ—Ç—Ä ("—Ñ–∏–∑–∏–∫–∞")
    with gr.Blocks() as demo:
        summarizer = LLMInterfacePro(
            typical_prompts={
                "–ü–∏—Å–∞—Ç–µ–ª—å": "–¢—ã —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤—ã–π –ø–∏—Å–∞—Ç–µ–ª—å –∏ —Ä–∞—Å—Å–∫–∞–∑—á–∏–∫. –ü–∏—à–∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ, –∂–∏–≤–æ –∏ —Å –¥–µ—Ç–∞–ª—è–º–∏, —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏—Ç–∞—Ç–µ–ª—è. –°–æ—Å—Ç–∞–≤—å {param} –∏–∑ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.",
            }, 
            prompt_params={
                "–ü–∏—Å–∞—Ç–µ–ª—å": ["–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ", "—Ä–∞—Å—Å–∫–∞–∑", "—ç—Å—Å–µ", "—Å—Ç–∞—Ç—å—è", "–ø–æ—ç–º–∞"],
            },
            default_prompt_index=0, 
            default_param_index=0
        )
        
    demo.launch()