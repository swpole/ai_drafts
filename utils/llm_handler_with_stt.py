import requests
import subprocess
import os
import gradio as gr

from textbox_with_stt_final import TextboxWithSTT

class LLMHandler:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama
    """
    
    def __init__(self):
        self.available_models = self.get_installed_ollama_models()
    
    def get_installed_ollama_models(self):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama —á–µ—Ä–µ–∑ API.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π.
        """
        models_list = []
        
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ API Ollama
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models_list = [model["name"] for model in data.get("models", [])]
                print(f"–ù–∞–π–¥–µ–Ω—ã –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ API: {models_list}")
                return models_list
        except requests.exceptions.RequestException as e:
            print(f"API Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {e}")
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É
            pass
        
        try:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥—É ollama list
            result = subprocess.run(["ollama", "list"], 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=10)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                for line in lines[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    if line.strip():
                        model_name = line.split()[0]
                        models_list.append(model_name)
                print(f"–ù–∞–π–¥–µ–Ω—ã –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ CLI: {models_list}")
                return models_list
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            print(f"–ö–æ–º–∞–Ω–¥–∞ ollama list –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
            pass
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª–∏. –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫.")
        return ["llama3.1", "gemma2", "mistral"]
    
    def check_ollama_running(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä Ollama.
        """
        try:
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def start_ollama_server(self):
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä Ollama.
        """
        try:
            print("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ Ollama...")
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –û–°)
            if os.name == 'nt':  # Windows
                subprocess.Popen(["ollama", "serve"], 
                               creationflags=subprocess.CREATE_NO_WINDOW)
            else:  # Linux/Mac
                subprocess.Popen(["ollama", "serve"], 
                               stdout=subprocess.DEVNULL, 
                               stderr=subprocess.DEVNULL)
            
            # –î–∞–µ–º —Å–µ—Ä–≤–µ—Ä—É –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—É—Å–∫
            import time
            time.sleep(3)
            return self.check_ollama_running()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ Ollama: {e}")
            return False
    
    def query_ollama(self, model_name, prompt, system_prompt="", temperature=0.7, max_tokens=1000):
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å (prompt) –≤ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Ollama –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            prompt: —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—Ä–æ–ª—å –º–æ–¥–µ–ª–∏)
            temperature: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0-1)
            max_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if not prompt.strip():
            return "–ó–∞–ø—Ä–æ—Å –ø—É—Å—Ç. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç."
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä Ollama
        if not self.check_ollama_running():
            if not self.start_ollama_server():
                return "–°–µ—Ä–≤–µ—Ä Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –≤—Ä—É—á–Ω—É—é."

        print(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –º–æ–¥–µ–ª–∏ {model_name}")
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π
            full_prompt = prompt
            if system_prompt.strip():
                full_prompt = f"{system_prompt}\n\n–ó–∞–ø—Ä–æ—Å: {prompt}"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º API Ollama –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            payload = {
                "model": model_name,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            }
            
            response = requests.post("http://localhost:11434/api/generate", 
                                   json=payload, 
                                   timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "–û—Ç–≤–µ—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω")
            else:
                return f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}"
                
        except requests.exceptions.Timeout:
            return "–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        except requests.exceptions.RequestException as e:
            return f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Ollama: {str(e)}"
        except Exception as e:
            return f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
    
    def refresh_models(self):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.
        """
        self.available_models = self.get_installed_ollama_models()
        return gr.Dropdown(choices=self.available_models)
    
    def create_interface(self):
        """
        –°–æ–∑–¥–∞–µ—Ç Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM
        """
        with gr.Blocks(title="LLM Interface") as interface:
            gr.Markdown("# ü§ñ –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ LLM –º–æ–¥–µ–ª—è–º–∏")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama
            ollama_status = "üü¢ –ó–∞–ø—É—â–µ–Ω" if self.check_ollama_running() else "üî¥ –ù–µ –∑–∞–ø—É—â–µ–Ω"
            gr.Markdown(f"**–°—Ç–∞—Ç—É—Å Ollama:** {ollama_status}")

            with gr.Row():
                # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ Ollama —Å –∫–Ω–æ–ø–∫–æ–π –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                with gr.Column(scale=3):
                    model_dropdown = gr.Dropdown(
                        choices=self.available_models, 
                        value=self.available_models[0] if self.available_models else None,
                        label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å Ollama",
                        interactive=True
                    )
                with gr.Column(scale=1):
                    refresh_button = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")

            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—Ä–æ–ª—å –º–æ–¥–µ–ª–∏)
            system_prompt_stt = TextboxWithSTT()
            system_prompt = system_prompt_stt.render(
                label="–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—Ä–æ–ª—å –º–æ–¥–µ–ª–∏)",
                placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –¢—ã - –æ–ø—ã—Ç–Ω—ã–π –ø–æ–ª–∏—Ç–æ–ª–æ–≥. –û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ...",
                lines=3,
                value="–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ.",
                show_copy_button=True
            )


            # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ –∑–∞–¥–∞–Ω–∏—è
            with gr.Row():
                user_prompt_stt = TextboxWithSTT()
                user_prompt = user_prompt_stt.render(
                    label="–ó–∞–¥–∞–Ω–∏–µ –¥–ª—è LLM",
                    placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –∑–¥–µ—Å—å...",
                    lines=5,
                    show_copy_button=True
                )


            # –ê–∫–∫–æ—Ä–¥–µ–æ–Ω –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            with gr.Accordion("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏", open=False):
                with gr.Row():
                    temperature_slider = gr.Slider(
                        minimum=0.1, 
                        maximum=1.0, 
                        value=0.7, 
                        step=0.1,
                        label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)",
                        info="–ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º–∏ –±—É–¥—É—Ç –æ—Ç–≤–µ—Ç—ã"
                    )
                    max_tokens_slider = gr.Slider(
                        minimum=100, 
                        maximum=4000, 
                        value=1000, 
                        step=100,
                        label="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤",
                        info="–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"
                    )

            # –ö–Ω–æ–ø–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
            submit_button = gr.Button("üöÄ –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å", variant="primary")

            # –ë–ª–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏
            response_output_stt = TextboxWithSTT()
            response_output = response_output_stt.render(
                label="–û—Ç–≤–µ—Ç LLM", 
                lines=10, 
                interactive=True,
                show_copy_button=True
            )

            # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM
            submit_button.click(
                fn=self.query_ollama, 
                inputs=[
                    model_dropdown, 
                    user_prompt, 
                    system_prompt,
                    temperature_slider,
                    max_tokens_slider
                ], 
                outputs=response_output
            )

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
            refresh_button.click(
                fn=self.refresh_models,
                outputs=model_dropdown
            )

            # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            with gr.Accordion("–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤", open=False):
                gr.Markdown("""
                **–ü—Ä–∏–º–µ—Ä—ã —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤:**
                - –¢—ã - –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç. –ü–æ–º–æ–≥–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ–¥ –∏ –æ–±—ä—è—Å–Ω–∏ —Ä–µ—à–µ–Ω–∏—è.
                - –¢—ã - –∏—Å—Ç–æ—Ä–∏–∫. –†–∞—Å—Å–∫–∞–∂–∏ —Ñ–∞–∫—Ç—ã —Ç–æ—á–Ω–æ –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ.
                - –¢—ã - –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –ø–∏—Å–∞—Ç–µ–ª—å. –ü—Ä–∏–¥—É–º–∞–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é.
                
                **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞–Ω–∏–π:**
                - –ù–∞–ø–∏—à–∏ Python —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å–ø–∏—Å–∫–∞
                - –û–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—É—é —Ñ–∏–∑–∏–∫—É –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
                - –ü—Ä–∏–¥—É–º–∞–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞
                """)

        return interface

    def launch_interface(self, server_name="0.0.0.0", share=False, debug=True):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        """
        interface = self.create_interface()
        
        print("–ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ LLMHandler...")
        print("–û—Ç–∫—Ä–æ–π—Ç–µ http://localhost:7860 –≤ –≤–∞—à–µ–º –±—Ä–∞—É–∑–µ—Ä–µ")
        
        try:
            interface.launch(
                server_name=server_name,
                share=share,
                debug=debug
            )
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞: {e}")
            print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø–æ—Ä—Ç 7860 —Å–≤–æ–±–æ–¥–µ–Ω –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –¥—Ä—É–≥–æ–π –ø–æ—Ä—Ç")

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM
    llm_handler = LLMHandler()
    
    print("=== LLMHandler Interface ===")
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {llm_handler.available_models}")
    print(f"–°–µ—Ä–≤–µ—Ä Ollama –∑–∞–ø—É—â–µ–Ω: {llm_handler.check_ollama_running()}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
    if not llm_handler.check_ollama_running():
        print("–°–µ—Ä–≤–µ—Ä Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ü—ã—Ç–∞—é—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å...")
        if llm_handler.start_ollama_server():
            print("–°–µ—Ä–≤–µ—Ä Ollama —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä Ollama. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    llm_handler.launch_interface()