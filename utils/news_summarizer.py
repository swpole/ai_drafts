import gradio as gr
import feedparser
import requests
import subprocess
from duckduckgo_search import DDGS
from newspaper import Article
from bs4 import BeautifulSoup

class NewsSummarizer:
    def __init__(self):
        self.titles_links_global = []
        self.RSS_FEEDS = {
            "BBC": "http://feeds.bbci.co.uk/news/rss.xml",
            "Reuters": "http://feeds.reuters.com/reuters/topNews",
            "CNN": "http://rss.cnn.com/rss/edition.rss",
            "–£–ù–ò–ê–ù": "https://rss.unian.net/site/news_ukr.rss"
        }

    # ---------- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ----------
    def search_duckduckgo(self, query, num=10):
        results = []
        with DDGS() as ddgs:
            for r in ddgs.news(query, max_results=num):
                results.append({
                    "title": r.get("title"),
                    "link": r.get("url"),
                    "source": r.get("source"),
                    "date": r.get("date")
                })
        return results

    def search_newsapi(self, query, num=10, api_key="YOUR_API_KEY"):
        url = f"https://newsapi.org/v2/everything?q={query}&pageSize={num}&apiKey={api_key}"
        r = requests.get(url).json()
        results = []
        for article in r.get("articles", []):
            results.append({
                "title": article.get("title"),
                "link": article.get("url"),
                "source": article.get("source", {}).get("name"),
                "date": article.get("publishedAt")
            })
        return results

    def search_rss(self, num=10):
        results = []
        for name, url in self.RSS_FEEDS.items():
            feed = feedparser.parse(url)
            for entry in feed.entries[:num//len(self.RSS_FEEDS)]:
                results.append({
                    "title": entry.get("title"),
                    "link": entry.get("link"),
                    "source": name,
                    "date": entry.get("published", "")
                })
        return results

    def search_news(self, query, method):
        if method == "DuckDuckGo":
            return self.search_duckduckgo(query)
        elif method == "NewsAPI":
            return self.search_newsapi(query)
        elif method == "RSS":
            return self.search_rss()
        else:
            return [{"title": "–û—à–∏–±–∫–∞: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥", "link": "", "source": "", "date": ""}]

    # ---------- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
    def format_results(self, results):
        if not results:
            self.titles_links_global = []
            return "<b>–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.</b>", []
        html = "<ol>"
        self.titles_links_global = []
        news_titles = []
        for r in results:
            title = r['title'] or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            source = r['source'] or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"
            date = r['date'] or ""
            link = r['link'] or "#"
            html += f"<li><a href='{link}' target='_blank'>{title}</a> <i>({source}, {date})</i></li>"
            self.titles_links_global.append((title, link))
            news_titles.append(title)
        html += "</ol>"
        return html, news_titles

    # ---------- –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Ollama ----------
    def get_ollama_models(self):
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            models = []
            for line in result.stdout.splitlines()[1:]:
                parts = line.split()
                if parts:
                    models.append(parts[0])
            return models if models else ["–ù–µ—Ç –º–æ–¥–µ–ª–µ–π"]
        except Exception as e:
            return [f"–û—à–∏–±–∫–∞: {e}"]

    # ---------- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ ----------
    def extract_article_text(self, url):
        try:
            article = Article(
                url,
                language='ru',
                browser_user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                                    "Chrome/120.0.0.0 Safari/537.36"))
            article.download()
            article.parse()
            if article.text and len(article.text.split()) > 50:
                return article.text
        except Exception:
            pass

        try:
            headers = {"User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                                      "Chrome/120.0.0.0 Safari/537.36")}
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")

            paragraphs = [p.get_text() for p in soup.find_all("p")]
            text = "\n".join(paragraphs).strip()

            if text and len(text.split()) > 30:
                return text
            else:
                return "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (—Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞)"
        except Exception as e:
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏: {e}"

    # ---------- –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ ----------
    def summarize_news(self, news_text, prompt, llm_model):
        try:
            full_prompt = f"{prompt}\n\n{news_text}"
            result = subprocess.run(
                ["ollama", "run", llm_model],
                input=full_prompt,
                text=True,
                capture_output=True
            )
            return result.stdout.strip() if result.stdout else "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {e}"

    # ---------- UI ----------
    def create_interface(self):
        with gr.Blocks() as interface:
            gr.Markdown("## üì∞ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π")

            with gr.Row():
                topic = gr.Textbox(label="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É", value="–í–æ–π–Ω–∞ –≤ –£–∫—Ä–∞–∏–Ω–µ")
                method = gr.Radio(["DuckDuckGo", "NewsAPI", "RSS"], value="DuckDuckGo", label="–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞")

            search_btn = gr.Button("–ü–æ–∏—Å–∫")
            output = gr.HTML(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            
            news_dropdown = gr.Dropdown(choices=[], label="–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è")
            llm_dropdown = gr.Dropdown(choices=self.get_ollama_models(), label="–í—ã–±–µ—Ä–∏—Ç–µ LLM (Ollama)", value=None)
            prompt_input = gr.Textbox(label="–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ", 
                                    value="–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–æ–µ –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –≤–∏–¥–µ–æ YouTube.", lines=3)
            
            extract_btn = gr.Button("–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏")
            extract_output = gr.Textbox(label="–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏", lines=10)
            
            summarize_btn = gr.Button("–°–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—é–º–µ")
            self.summary_output = gr.Textbox(label="–†–µ–∑—é–º–µ", lines=10, interactive=True)

            # ---------- –õ–æ–≥–∏–∫–∞ ----------
            def search_and_update_dropdown(query, method, dropdown):
                html, news_titles = self.format_results(self.search_news(query, method))
                return html, gr.update(choices=news_titles, value=news_titles[0] if news_titles else None)

            search_btn.click(fn=search_and_update_dropdown, 
                            inputs=[topic, method, news_dropdown], 
                            outputs=[output, news_dropdown])

            def extract_selected(news_choice, prompt, llm_model):
                if not news_choice or not self.titles_links_global:
                    return "–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –∏ –º–æ–¥–µ–ª—å"
                link = None
                for title, lnk in self.titles_links_global:
                    if title == news_choice:
                        link = lnk
                        break
                if not link:
                    return "–°—Å—ã–ª–∫–∞ –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                news_text = self.extract_article_text(link)
                return news_text

            extract_btn.click(fn=extract_selected, 
                            inputs=[news_dropdown, prompt_input, llm_dropdown],
                            outputs=[extract_output])
            
            def summarize_text(news_text, prompt, llm_model):
                return self.summarize_news(news_text, prompt, llm_model)

            summarize_btn.click(fn=summarize_text, 
                                inputs=[extract_output, prompt_input, llm_dropdown],
                                outputs=[self.summary_output])    

        return interface


if __name__ == "__main__":
    news_summarizer = NewsSummarizer()
    interface = news_summarizer.create_interface()
    interface.launch()
