import gradio as gr
import ollama
import subprocess

class LLMInterface:
    def __init__(
        self,
        title: str = "Title",
        heading: str = "Heading",
        prompt_label: str = "–ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏",
        prompt_default: str = "Prompt for the model",
        input_label: str = "Label input",
        input_placeholder: str = "–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ —Ç–µ–∫—Å—Ç ...",
        input_value: str = "Hi! How are you?",
        generate_button_text: str = "Label button",
        output_label: str = "Label output",
    ):
 
        self.title = title
        self.heading = heading
        self.prompt_label = prompt_label
        self.prompt_default = prompt_default
        self.input_label = input_label
        self.input_placeholder = input_placeholder
        self.input_value = input_value
        self.generate_button_text = generate_button_text
        self.output_label = output_label

    def get_models(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama."""
        try:
            # –≠—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç Ollama –µ—Å–ª–∏ –Ω–µ –±—ã–ª–∞ –∑–∞–ø—É—â–µ–Ω–∞
            subprocess.run(["ollama", "list"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            models_info = ollama.list()
            return [m["model"] for m in models_info["models"]]
        except Exception as e:
            err = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π:{e} –í–æ–∑–º–æ–∂–Ω–æ ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'ollama list' –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ."
            return [err]

    def generate(self, model_name, prompt, input_text):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç ollama —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é."""
        if not input_text or not input_text.strip():
            return "‚ö†Ô∏è " + self.input_label + " –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º."
        try:
            full_prompt = f"{prompt}\n\n{self.input_label}:\n{input_text}"
            response = ollama.chat(
                model=model_name,
                messages=[{"role": "user", "content": full_prompt}]
            )
            return response["message"]["content"]
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"

    def build_interface(self):
        models = self.get_models()
        with gr.Blocks(title=self.title) as demo:
            gr.Markdown(f"## üìù {self.heading}")

            with gr.Row():
                model_dropdown = gr.Dropdown(
                    choices=models,
                    value=models[0] if models else None,
                    label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å"
                )

            prompt_box = gr.Textbox(
                label=self.prompt_label,
                value=self.prompt_default,
                lines=1
            )

            input_box = gr.Textbox(
                label=self.input_label,
                placeholder=self.input_placeholder,
                value=self.input_value,
                lines=5
            )

            run_button = gr.Button(self.generate_button_text)

            output_box = gr.Textbox(
                label=self.output_label,
                lines=5
            )

            run_button.click(
                fn=self.generate,
                inputs=[model_dropdown, prompt_box, input_box],
                outputs=output_box
            )

        return demo

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    summarizer = LLMInterface()
    demo = summarizer.build_interface()
    demo.launch()
