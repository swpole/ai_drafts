import os
import pickle
import gradio as gr
import numpy as np
import ollama
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ===============================
# 1Ô∏è‚É£ DocumentStore
# ===============================
class DocumentStore:
    def __init__(self, storage_dir="debug/databases"):
        self.documents = {}
        self.storage_dir = storage_dir
        os.makedirs(self.storage_dir, exist_ok=True)

    def add_document(self, file_path):
        if not file_path:
            return self.list_documents()
        file_name = os.path.basename(file_path)
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()
        self.documents[file_name] = content
        return self.list_documents()

    def remove_document(self, name):
        if name in self.documents:
            del self.documents[name]
        return self.list_documents()

    def list_documents(self):
        if not self.documents:
            return "(–ë–∞–∑–∞ –ø—É—Å—Ç–∞)"
        return "\n".join([f"{name}: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤" for name, text in self.documents.items()])

    def save_database(self, name):
        path = os.path.join(self.storage_dir, name + ".pkl")
        with open(path, "wb") as f:
            pickle.dump(self.documents, f)
        return f"‚úÖ –ë–∞–∑–∞ '{name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞."

    def load_database(self, name):
        path = os.path.join(self.storage_dir, name + ".pkl")
        if not os.path.exists(path):
            return f"‚ùå –ë–∞–∑–∞ '{name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        with open(path, "rb") as f:
            self.documents = pickle.load(f)
        return f"‚úÖ –ë–∞–∑–∞ '{name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞."

    def get_all_texts(self):
        return list(self.documents.values())

    def get_document_names(self):
        return list(self.documents.keys())


# ===============================
# 2Ô∏è‚É£ QueryPreprocessor
# ===============================
class QueryPreprocessor:
    def __init__(self, lowercase=True, remove_punct=True):
        self.lowercase = lowercase
        self.remove_punct = remove_punct

    def preprocess(self, query: str):
        text = query.strip()
        if self.lowercase:
            text = text.lower()
        if self.remove_punct:
            import re
            text = re.sub(r"[^\w\s]", "", text)
        return text


# ===============================
# 3Ô∏è‚É£ DocumentSearcher
# ===============================
class DocumentSearcher:
    def __init__(self, store: DocumentStore):
        self.store = store
        self.embedding_provider = "tfidf"
        self.vectorizer = None
        self.doc_vectors = None
        self.model_st = None

    def set_embedding_provider(self, provider):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–± –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        self.embedding_provider = provider
        return f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ {provider}"

    def build_index(self):
        texts = self.store.get_all_texts()
        if not texts:
            return "‚ö†Ô∏è –í –±–∞–∑–µ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."

        if self.embedding_provider == "tfidf":
            try:
                self.vectorizer = TfidfVectorizer(stop_words="english")
                self.doc_vectors = self.vectorizer.fit_transform(texts).toarray()
            except Exception as e:
                return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ TF-IDF –∏–Ω–¥–µ–∫—Å–∞: {e}"
            return f"‚úÖ TF-IDF –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        elif self.embedding_provider == "sentence-transformers":
            try:
                from sentence_transformers import SentenceTransformer
                self.model_st = SentenceTransformer("all-MiniLM-L6-v2")
                self.doc_vectors = self.model_st.encode(texts, convert_to_numpy=True)
            except Exception as e:
                return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç sentence-transformers: {e}"
            
            return f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ Sentence Transformers ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        elif self.embedding_provider == "ollama":
            embeddings = []
            for text in texts:
                try:
                    resp = ollama.embeddings(model="nomic-embed-text", prompt=text)
                    embeddings.append(resp["embedding"])                
                except Exception as e:
                    return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç Ollama: {e}"

            self.doc_vectors = np.array(embeddings)
            return f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ Ollama Embeddings ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        return "‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."

    def embed_query(self, query):
        if self.embedding_provider == "tfidf":
            return self.vectorizer.transform([query]).toarray()
        elif self.embedding_provider == "sentence-transformers":
            return self.model_st.encode([query], convert_to_numpy=True)
        elif self.embedding_provider == "ollama":
            resp = ollama.embeddings(model="nomic-embed-text", prompt=query)
            return np.array([resp["embedding"]])
        else:
            raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")

    def search(self, query: str, top_k=3):
        if self.doc_vectors is None:
            return "‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–∂–º–∏ '–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å'."
        query_vec = self.embed_query(query)
        similarities = cosine_similarity(query_vec, self.doc_vectors).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]
        results = []
        for idx in top_indices:
            doc_name = self.store.get_document_names()[idx]
            score = similarities[idx]
            snippet = self.store.documents[doc_name][:300].replace("\n", " ")
            results.append((doc_name, round(float(score), 3), snippet))
        return results


# ===============================
# 4Ô∏è‚É£ Gradio UI
# ===============================
store = DocumentStore()
preprocessor = QueryPreprocessor()
searcher = DocumentSearcher(store)

with gr.Blocks(title="RAG Document Search (Embeddings Switch)") as demo:
    gr.Markdown("## üß† –ü–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É —Å –≤—ã–±–æ—Ä–æ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç (.txt)", file_types=[".txt"], type="filepath")
            add_btn = gr.Button("–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")

            provider = gr.Radio(
                ["tfidf", "sentence-transformers", "ollama"],
                label="–ü—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
                value="tfidf"
            )
            build_btn = gr.Button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å")

            query_input = gr.Textbox(label="–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç ollama.chat?")
            search_btn = gr.Button("–ü–æ–∏—Å–∫")

        with gr.Column(scale=2):
            output_box = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã", lines=15)

    # --- –õ–æ–≥–∏–∫–∞ ---
    add_btn.click(lambda f: store.add_document(f), inputs=[file_input], outputs=[output_box])
    provider.change(lambda p: searcher.set_embedding_provider(p), inputs=[provider], outputs=[output_box])
    build_btn.click(lambda: searcher.build_index(), outputs=[output_box])

    def on_search(query):
        clean = preprocessor.preprocess(query)
        results = searcher.search(clean)
        if isinstance(results, str):
            return results
        text = "\n\n".join([f"üìÑ {name} ({score}):\n{snippet}" for name, score, snippet in results])
        return text

    search_btn.click(on_search, inputs=[query_input], outputs=[output_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", share=False)
