import os
import pickle
import gradio as gr
import numpy as np
import ollama
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ===============================
# 1Ô∏è‚É£ DocumentStore
# ===============================
class DocumentStore:
    def __init__(self, storage_dir="databases"):
        self.documents = {}
        self.storage_dir = storage_dir
        os.makedirs(self.storage_dir, exist_ok=True)

    def add_document(self, file_path):
        if not file_path:
            return self.list_documents()
        file_name = os.path.basename(file_path)
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()
        self.documents[file_name] = content
        return self.list_documents()

    def list_documents(self):
        if not self.documents:
            return "(–ë–∞–∑–∞ –ø—É—Å—Ç–∞)"
        return "\n".join([f"{name}: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤" for name, text in self.documents.items()])

    def save_database(self, name):
        path = os.path.join(self.storage_dir, name + ".pkl")
        with open(path, "wb") as f:
            pickle.dump(self.documents, f)
        return f"‚úÖ –ë–∞–∑–∞ '{name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞."

    def get_all_texts(self):
        return list(self.documents.values())

    def get_document_names(self):
        return list(self.documents.keys())


# ===============================
# 2Ô∏è‚É£ QueryPreprocessor
# ===============================
class QueryPreprocessor:
    def __init__(self, lowercase=True, remove_punct=True):
        self.lowercase = lowercase
        self.remove_punct = remove_punct

    def preprocess(self, query: str):
        text = query.strip()
        if self.lowercase:
            text = text.lower()
        if self.remove_punct:
            import re
            text = re.sub(r"[^\w\s]", "", text)
        return text


# ===============================
# 3Ô∏è‚É£ DocumentSearcher
# ===============================
class DocumentSearcher:
    def __init__(self, store: DocumentStore):
        self.store = store
        self.embedding_provider = "tfidf"
        self.vectorizer = None
        self.doc_vectors = None
        self.model_st = None

    def set_embedding_provider(self, provider):
        self.embedding_provider = provider
        return f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ {provider}"

    def build_index(self):
        texts = self.store.get_all_texts()
        if not texts:
            return "‚ö†Ô∏è –í –±–∞–∑–µ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."

        if self.embedding_provider == "tfidf":
            self.vectorizer = TfidfVectorizer(stop_words="russian")
            self.doc_vectors = self.vectorizer.fit_transform(texts).toarray()
            return f"‚úÖ TF-IDF –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        elif self.embedding_provider == "sentence-transformers":
            try:
                from sentence_transformers import SentenceTransformer
                self.model_st = SentenceTransformer("all-MiniLM-L6-v2")
            except ImportError:
                return "‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏ –ø–∞–∫–µ—Ç sentence-transformers."
            self.doc_vectors = self.model_st.encode(texts, convert_to_numpy=True)
            return f"‚úÖ –ò–Ω–¥–µ–∫—Å —á–µ—Ä–µ–∑ Sentence Transformers ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        elif self.embedding_provider == "ollama":
            embeddings = []
            for text in texts:
                resp = ollama.embeddings(model="nomic-embed-text", prompt=text)
                embeddings.append(resp["embedding"])
            self.doc_vectors = np.array(embeddings)
            return f"‚úÖ –ò–Ω–¥–µ–∫—Å —á–µ—Ä–µ–∑ Ollama Embeddings ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

    def embed_query(self, query):
        if self.embedding_provider == "tfidf":
            return self.vectorizer.transform([query]).toarray()
        elif self.embedding_provider == "sentence-transformers":
            return self.model_st.encode([query], convert_to_numpy=True)
        elif self.embedding_provider == "ollama":
            resp = ollama.embeddings(model="nomic-embed-text", prompt=query)
            return np.array([resp["embedding"]])

    def search(self, query: str, top_k=3):
        if self.doc_vectors is None:
            return "‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω."
        query_vec = self.embed_query(query)
        similarities = cosine_similarity(query_vec, self.doc_vectors).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]
        results = []
        for idx in top_indices:
            doc_name = self.store.get_document_names()[idx]
            score = similarities[idx]
            snippet = self.store.documents[doc_name][:500].replace("\n", " ")
            results.append((doc_name, round(float(score), 3), snippet))
        return results


# ===============================
# 4Ô∏è‚É£ AnswerGenerator
# ===============================
class AnswerGenerator:
    def __init__(self):
        self.gemini_models = ["gemini-2.0-flash", "gemini-2.0-pro"]
        self.ollama_models = self._load_ollama_models()

    def _load_ollama_models(self):
        try:
            models = ollama.list()
            return [m["model"] for m in models["models"]]
        except Exception:
            return ["llama3", "mistral", "phi3"]

    def generate_answer(self, llm_provider, model, query, context_texts):
        context = "\n\n".join([f"---\n{c}" for c in context_texts])
        prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç, –æ–ø–∏—Ä–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç."

        if llm_provider == "ollama":
            try:
                resp = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])
                return resp["message"]["content"].strip()
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ Ollama] {e}"

        elif llm_provider == "gemini":
            try:
                import google.generativeai as genai
                genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
                model_obj = genai.GenerativeModel(model)
                resp = model_obj.generate_content(prompt)
                return resp.text.strip()
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ Gemini] {e}"

        return "[–û—à–∏–±–∫–∞] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM."


# ===============================
# 5Ô∏è‚É£ Gradio UI
# ===============================
store = DocumentStore()
preprocessor = QueryPreprocessor()
searcher = DocumentSearcher(store)
generator = AnswerGenerator()

with gr.Blocks(title="RAG v3 ‚Äî Semantic Search + Answer Generation") as demo:
    gr.Markdown("## ü§ñ Retrieval-Augmented Generation (RAG)\n–î–æ–±–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –∏–Ω–¥–µ–∫—Å ‚Üí –ù–∞–π–¥–∏—Ç–µ ‚Üí –°–æ—Å—Ç–∞–≤—å—Ç–µ –æ—Ç–≤–µ—Ç")

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç (.txt)", file_types=[".txt"], type="filepath")
            add_btn = gr.Button("–î–æ–±–∞–≤–∏—Ç—å")

            provider = gr.Radio(["tfidf", "sentence-transformers", "ollama"], label="–ü—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", value="tfidf")
            build_btn = gr.Button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å")

            query_input = gr.Textbox(label="–ó–∞–ø—Ä–æ—Å")
            search_btn = gr.Button("üîç –ü–æ–∏—Å–∫")

            llm_provider = gr.Radio(["ollama", "gemini"], label="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–∞", value="ollama")
            ollama_models = gr.Dropdown(choices=generator.ollama_models, label="–ú–æ–¥–µ–ª—å Ollama", value=generator.ollama_models[0])
            gemini_models = gr.Dropdown(choices=generator.gemini_models, label="–ú–æ–¥–µ–ª—å Gemini", value=generator.gemini_models[0])

            answer_btn = gr.Button("üí¨ –°–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–≤–µ—Ç")

        with gr.Column(scale=2):
            search_results = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞", lines=10)
            final_answer = gr.Textbox(label="–û—Ç–≤–µ—Ç", lines=10)

    # --- Handlers ---
    add_btn.click(lambda f: store.add_document(f), inputs=[file_input], outputs=[search_results])
    provider.change(lambda p: searcher.set_embedding_provider(p), inputs=[provider], outputs=[search_results])
    build_btn.click(lambda: searcher.build_index(), outputs=[search_results])

    def on_search(query):
        clean = preprocessor.preprocess(query)
        results = searcher.search(clean)
        if isinstance(results, str):
            return results
        text = "\n\n".join([f"üìÑ {name} ({score}):\n{snippet}" for name, score, snippet in results])
        return text

    search_btn.click(on_search, inputs=[query_input], outputs=[search_results])

    def on_answer(query, llm_provider, ollama_model, gemini_model):
        results = searcher.search(query)
        if isinstance(results, str):
            return results
        context_texts = [snippet for _, _, snippet in results]
        model = ollama_model if llm_provider == "ollama" else gemini_model
        answer = generator.generate_answer(llm_provider, model, query, context_texts)
        return answer

    answer_btn.click(
        on_answer,
        inputs=[query_input, llm_provider, ollama_models, gemini_models],
        outputs=[final_answer]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", share=False)
