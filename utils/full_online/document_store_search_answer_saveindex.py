import os
import pickle
import gradio as gr
import numpy as np
import ollama
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ===============================
# 1Ô∏è‚É£ DocumentStore
# ===============================
class DocumentStore:
    def __init__(self, storage_dir="debug/databases"):
        self.documents = {}
        self.storage_dir = storage_dir
        os.makedirs(self.storage_dir, exist_ok=True)

    def add_document(self, file_path):
        if not file_path:
            return self.list_documents()
        file_name = os.path.basename(file_path)
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()
        self.documents[file_name] = content
        return self.list_documents()

    def list_documents(self):
        if not self.documents:
            return "(–ë–∞–∑–∞ –ø—É—Å—Ç–∞)"
        return "\n".join([f"{name}: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤" for name, text in self.documents.items()])

    def save_database(self, name):
        path = os.path.join(self.storage_dir, name + ".pkl")
        with open(path, "wb") as f:
            pickle.dump(self.documents, f)
        return f"‚úÖ –ë–∞–∑–∞ '{name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞."

    def load_database(self, name):
        path = os.path.join(self.storage_dir, name + ".pkl")
        if not os.path.exists(path):
            return "‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω."
        with open(path, "rb") as f:
            self.documents = pickle.load(f)
        return f"‚úÖ –ë–∞–∑–∞ '{name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

    def get_all_texts(self):
        return list(self.documents.values())

    def get_document_names(self):
        return list(self.documents.keys())


# ===============================
# 2Ô∏è‚É£ QueryPreprocessor
# ===============================
class QueryPreprocessor:
    def preprocess(self, query: str):
        text = query.strip().lower()
        import re
        text = re.sub(r"[^\w\s]", "", text)
        return text


# ===============================
# 3Ô∏è‚É£ DocumentSearcher (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–Ω–¥–µ–∫—Å–∞)
# ===============================
class DocumentSearcher:
    def __init__(self, store: DocumentStore, index_dir="debug/indexes"):
        self.store = store
        self.index_dir = index_dir
        os.makedirs(self.index_dir, exist_ok=True)

        self.embedding_provider = "tfidf"
        self.vectorizer = None
        self.model_st = None
        self.doc_vectors = None

    def set_embedding_provider(self, provider):
        self.embedding_provider = provider
        return f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ {provider}"

    def build_index(self):
        texts = self.store.get_all_texts()
        if not texts:
            return "‚ö†Ô∏è –í –±–∞–∑–µ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."

        if self.embedding_provider == "tfidf":
            self.vectorizer = TfidfVectorizer(stop_words="russian")
            self.doc_vectors = self.vectorizer.fit_transform(texts).toarray()
            return f"‚úÖ TF-IDF –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        elif self.embedding_provider == "sentence-transformers":
            try:
                from sentence_transformers import SentenceTransformer
                self.model_st = SentenceTransformer("all-MiniLM-L6-v2")
            except ImportError:
                return "‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏ –ø–∞–∫–µ—Ç sentence-transformers."
            self.doc_vectors = self.model_st.encode(texts, convert_to_numpy=True)
            return f"‚úÖ Sentence Transformers –∏–Ω–¥–µ–∫—Å ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

        elif self.embedding_provider == "ollama":
            embeddings = []
            for text in texts:
                resp = ollama.embeddings(model="nomic-embed-text", prompt=text)
                embeddings.append(resp["embedding"])
            self.doc_vectors = np.array(embeddings)
            return f"‚úÖ Ollama Embeddings –∏–Ω–¥–µ–∫—Å ({len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."

    def embed_query(self, query):
        if self.embedding_provider == "tfidf":
            return self.vectorizer.transform([query]).toarray()
        elif self.embedding_provider == "sentence-transformers":
            return self.model_st.encode([query], convert_to_numpy=True)
        elif self.embedding_provider == "ollama":
            resp = ollama.embeddings(model="nomic-embed-text", prompt=query)
            return np.array([resp["embedding"]])

    def search(self, query: str, top_k=3):
        if self.doc_vectors is None:
            return "‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω."
        query_vec = self.embed_query(query)
        similarities = cosine_similarity(query_vec, self.doc_vectors).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]
        results = []
        for idx in top_indices:
            doc_name = self.store.get_document_names()[idx]
            score = similarities[idx]
            snippet = self.store.documents[doc_name][:500].replace("\n", " ")
            results.append((doc_name, round(float(score), 3), snippet))
        return results

    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ ===
    def save_index(self, name):
        path = os.path.join(self.index_dir, name + ".pkl")
        data = {
            "provider": self.embedding_provider,
            "vectorizer": self.vectorizer,
            "doc_vectors": self.doc_vectors,
            "doc_names": self.store.get_document_names(),
            "documents": self.store.documents,   # ‚úÖ –¥–æ–±–∞–≤–ª–µ–Ω–æ
        }
        with open(path, "wb") as f:
            pickle.dump(data, f)
        return f"‚úÖ –ò–Ω–¥–µ–∫—Å '{name}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω."

    def load_index(self, name):
        path = os.path.join(self.index_dir, name + ".pkl")
        if not os.path.exists(path):
            return "‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω."
        with open(path, "rb") as f:
            data = pickle.load(f)

        self.embedding_provider = data["provider"]
        self.vectorizer = data["vectorizer"]
        self.doc_vectors = data["doc_vectors"]
        self.store.documents = data.get("documents", {})   # ‚úÖ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã
        return f"‚úÖ –ò–Ω–¥–µ–∫—Å '{name}' –∑–∞–≥—Ä—É–∂–µ–Ω ({len(self.store.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."


    def list_indexes(self):
        files = [f[:-4] for f in os.listdir(self.index_dir) if f.endswith(".pkl")]
        return files or ["(–Ω–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤)"]


# ===============================
# 4Ô∏è‚É£ AnswerGenerator
# ===============================
class AnswerGenerator:
    def __init__(self):
        self.gemini_models = ["gemini-2.5-flash"]
        self.ollama_models = self._load_ollama_models()

    def _load_ollama_models(self):
        try:
            models = ollama.list()
            return [m["model"] for m in models["models"]]
        except Exception:
            return ["llama3", "mistral", "phi3"]

    def generate_answer(self, llm_provider, model, query, context_texts):
        context = "\n\n".join([f"---\n{c}" for c in context_texts])
        prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ, –æ–ø–∏—Ä–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç."

        if llm_provider == "ollama":
            try:
                resp = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])
                return resp["message"]["content"].strip()
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ Ollama] {e}"

        elif llm_provider == "gemini":
            try:
                from google import genai
                from google.genai import types
                client = genai.Client()

                config = types.GenerateContentConfig()

                system_instruction = config.system_instruction
                temperature = config.temperature
                top_p = config.top_p
                top_k = config.top_k
                max_output_tokens = config.max_output_tokens

                safety_settings = config.safety_settings
                safety_settings = [
                        types.SafetySetting(
                            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                        ),]

                config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_output_tokens=max_output_tokens,
                    safety_settings=safety_settings,
                )
                resp = client.models.generate_content(
                    model=model,
                    config=config,
                    contents=prompt,
                )
                return resp.text.strip()
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ Gemini] {e}"

        return "[–û—à–∏–±–∫–∞] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM."


# ===============================
# 5Ô∏è‚É£ Gradio UI
# ===============================
store = DocumentStore()
preprocessor = QueryPreprocessor()
searcher = DocumentSearcher(store)
generator = AnswerGenerator()

with gr.Blocks(title="RAG v4 ‚Äî —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–Ω–¥–µ–∫—Å–æ–≤") as demo:
    gr.Markdown("## üß† Retrieval-Augmented Generation (RAG v4)\n–î–æ–±–∞–≤–ª—è–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å—Ç—Ä–æ–π—Ç–µ –∏–Ω–¥–µ–∫—Å, —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∏ –æ—Ç–≤–µ—á–∞–π—Ç–µ.")

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç (.txt)", file_types=[".txt"], type="filepath")
            add_btn = gr.Button("–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")

            provider = gr.Radio(["tfidf", "sentence-transformers", "ollama"], label="–ú–µ—Ç–æ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", value="tfidf")
            build_btn = gr.Button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å")

            index_name = gr.Textbox(label="–ò–º—è –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            save_index_btn = gr.Button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å")

            index_dropdown = gr.Dropdown(label="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã", choices=searcher.list_indexes())
            refresh_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫")
            load_index_btn = gr.Button("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å")

            query_input = gr.Textbox(label="–ó–∞–ø—Ä–æ—Å", value="–ü—Ä–æ—Ç–∏–≤ –∫–∞–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –≤–≤–µ–ª–∏ –Ω–æ–≤—ã–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ —Å–∞–Ω–∫—Ü–∏–∏?")
            search_btn = gr.Button("üîç –ü–æ–∏—Å–∫")

            llm_provider = gr.Radio(["ollama", "gemini"], label="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–∞", value="ollama")
            ollama_models = gr.Dropdown(choices=generator.ollama_models, label="–ú–æ–¥–µ–ª—å Ollama", value=generator.ollama_models[0])
            gemini_models = gr.Dropdown(choices=generator.gemini_models, label="–ú–æ–¥–µ–ª—å Gemini", value=generator.gemini_models[0])

            answer_btn = gr.Button("üí¨ –°–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–≤–µ—Ç")

        with gr.Column(scale=2):
            output_box = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã", lines=20)

    # === Handlers ===
    add_btn.click(lambda f: store.add_document(f), inputs=[file_input], outputs=[output_box])
    provider.change(lambda p: searcher.set_embedding_provider(p), inputs=[provider], outputs=[output_box])
    build_btn.click(lambda: searcher.build_index(), outputs=[output_box])
    save_index_btn.click(lambda name: searcher.save_index(name), inputs=[index_name], outputs=[output_box])
    refresh_btn.click(lambda: gr.update(choices=searcher.list_indexes()), outputs=[index_dropdown])
    load_index_btn.click(lambda name: searcher.load_index(name), inputs=[index_dropdown], outputs=[output_box])

    def on_search(query):
        clean = preprocessor.preprocess(query)
        results = searcher.search(clean)
        if isinstance(results, str):
            return results
        return "\n\n".join([f"üìÑ {n} ({s}):\n{t}" for n, s, t in results])

    search_btn.click(on_search, inputs=[query_input], outputs=[output_box])

    def on_answer(query, llm_provider, ollama_model, gemini_model):
        results = searcher.search(query)
        if isinstance(results, str):
            return results
        context_texts = [snippet for _, _, snippet in results]
        model = ollama_model if llm_provider == "ollama" else gemini_model
        return generator.generate_answer(llm_provider, model, query, context_texts)

    answer_btn.click(on_answer, inputs=[query_input, llm_provider, ollama_models, gemini_models], outputs=[output_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", share=False)
