import gradio as gr
import ollama
import os
import tempfile
import torch
import torchaudio
from transformers import VitsModel, AutoTokenizer
import numpy as np
from scipy.io import wavfile
import io
import soundfile as sf
from typing import List, Dict, Tuple, Optional
import json
import pickle

# –î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞
import speech_recognition as sr
from pydub import AudioSegment

class VitsTTS:
    def __init__(self, model_name: str = "facebook/mms-tts-rus"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TTS"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = VitsModel.from_pretrained(self.model_name).to(self.device)
            print(f"–ú–æ–¥–µ–ª—å TTS {self.model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ TTS: {e}")
    
    def text_to_speech(self, text: str, speed: float = 1.0) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ—á—å —Å –ø–æ–º–æ—â—å—é VitsModel"""
        if not text.strip():
            return ""
        
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ
            with torch.no_grad():
                output = self.model(**inputs)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö
            audio_data = output.waveform.cpu().numpy()[0]
            sample_rate = self.model.config.sampling_rate
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if speed != 1.0:
                audio_data = self._change_speed(audio_data, sample_rate, speed)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                sf.write(tmp_file.name, audio_data, sample_rate)
                return tmp_file.name
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {e}")
            return ""
    
    def _change_speed(self, audio_data: np.ndarray, sample_rate: int, speed: float) -> np.ndarray:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∞—É–¥–∏–æ"""
        if speed == 1.0:
            return audio_data
            
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥
        new_length = int(len(audio_data) / speed)
        indices = np.linspace(0, len(audio_data) - 1, new_length)
        return np.interp(indices, np.arange(len(audio_data)), audio_data)

class RAGChatSystem:
    def __init__(self, model_name: str = "llama2"):
        self.model_name = model_name
        self.conversation_history = []
        self.vector_db = []
        
    def add_to_knowledge_base(self, documents: List[str]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        for doc in documents:
            self.vector_db.append({
                "content": doc,
                "embedding": self.get_embedding(doc)
            })
    
    def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            response = ollama.embeddings(model='nomic-embed-text', prompt=text)
            return response['embedding']
        except:
            # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
            return [len(text)] * 384
    
    def semantic_search(self, query: str, top_k: int = 3) -> List[str]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        if not self.vector_db:
            return []
            
        query_embedding = self.get_embedding(query)
        
        results = []
        for doc in self.vector_db:
            similarity = self.cosine_similarity(query_embedding, doc["embedding"])
            results.append((similarity, doc["content"]))
        
        results.sort(reverse=True)
        return [content for _, content in results[:top_k]]
    
    def cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        a = np.array(a)
        b = np.array(b)
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def generate_response(self, user_input: str) -> Tuple[str, str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        relevant_docs = self.semantic_search(user_input)
        context = "\n".join(relevant_docs) if relevant_docs else "–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_input}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —Ç–æ—á–Ω—ã–º."""
        
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=self.conversation_history + [
                    {"role": "system", "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            answer = response['message']['content']
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.conversation_history.append({"role": "user", "content": user_input})
            self.conversation_history.append({"role": "assistant", "content": answer})
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            if len(self.conversation_history) > 8:
                self.conversation_history = self.conversation_history[-8:]
            
            return answer, context
        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {str(e)}", ""
        
    def save_knowledge_base(self, filename: str = "knowledge_base.json"):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ —Ñ–∞–π–ª"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.vector_db, f, ensure_ascii=False, indent=2)
    
    def load_knowledge_base(self, filename: str = "knowledge_base.json"):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.vector_db = json.load(f)
        except FileNotFoundError:
            print("–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è")
    
    def save_with_pickle(self, filename: str = "knowledge_base.pkl"):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pickle (–¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)"""
        with open(filename, 'wb') as f:
            pickle.dump(self.vector_db, f)
    
    def load_with_pickle(self, filename: str = "knowledge_base.pkl"):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pickle"""
        try:
            with open(filename, 'rb') as f:
                self.vector_db = pickle.load(f)
        except FileNotFoundError:
            print("–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω")

class VoiceProcessor:
    def __init__(self):
        self.tts = VitsTTS()
    
    def speech_to_text(self, audio_file: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç"""
        try:
            recognizer = sr.Recognizer()
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if audio_file.endswith('.mp3'):
                audio = AudioSegment.from_mp3(audio_file)
                wav_file = audio_file.replace('.mp3', '.wav')
                audio.export(wav_file, format='wav')
                audio_file = wav_file
            
            with sr.AudioFile(audio_file) as source:
                audio_data = recognizer.record(source)
                text = recognizer.recognize_google(audio_data, language='ru-RU')
                return text
        except sr.UnknownValueError:
            return "–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {str(e)}"
    
    def text_to_speech(self, text: str, speed: float = 1.0) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é VITS"""
        return self.tts.text_to_speech(text, speed)



# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama
try:
    models = ollama.list()
    available_models = [model['model'] for model in models['models']]
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:", available_models)
    default_model = available_models[0] if available_models else "llama2"
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π:", e)
    available_models = ["llama2"]
    default_model = "llama2"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º
rag_system = RAGChatSystem(model_name=default_model)
voice_processor = VoiceProcessor()

rag_system.load_knowledge_base("knowledge_base.json")

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ (–µ—Å–ª–∏ –±–∞–∑–∞ –ø—É—Å—Ç–∞—è)
if not rag_system.vector_db:
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    example_documents = [
        "Ollama - —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–µ–π –ª–æ–∫–∞–ª—å–Ω–æ. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∫–∞–∫ Llama, Mistral –∏ –¥—Ä—É–≥–∏–µ –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.",
        "Gradio - —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ Python –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ—Å—Ç–∞ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ.",
        "RAG (Retrieval-Augmented Generation) - —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞, —Å–æ—á–µ—Ç–∞—é—â–∞—è –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞. –£–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏.",
        "VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏.",
        "–î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ SpeechRecognition, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –¥–≤–∏–∂–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è."
    ]
    rag_system.add_to_knowledge_base(example_documents)
    rag_system.save_knowledge_base()  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è

def chat_interface(message: str, audio_input=None, use_voice_input: bool = False, speech_speed: float = 1.0):
    """–û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á–∞—Ç–∞"""
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞
    if use_voice_input and audio_input is not None:
        user_input = voice_processor.speech_to_text(audio_input)
    else:
        user_input = message
    
    if not user_input.strip():
        return "–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ", "", None, ""
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    response, context = rag_system.generate_response(user_input)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –æ—Ç–≤–µ—Ç–∞
    audio_output = voice_processor.text_to_speech(response, speech_speed)
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–≤–æ–¥–∞
    formatted_output = f"""**–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:** {user_input}

**–û—Ç–≤–µ—Ç:** {response}

---
*–ö–æ–Ω—Ç–µ–∫—Å—Ç RAG:*
{context if context else '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏'}"""
    
    return formatted_output, response, audio_output if audio_output else None, user_input if use_voice_input else ""

def clear_chat():
    """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞"""
    rag_system.conversation_history.clear()
    return "", "", None, ""

def add_text_to_knowledge(text):
    """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    if text and text.strip():
        rag_system.add_to_knowledge_base([text.strip()])
        rag_system.save_knowledge_base()
        return f"–¢–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π! –†–∞–∑–º–µ—Ä –±–∞–∑—ã: {len(rag_system.vector_db)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
    return "–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, –Ω–µ—á–µ–≥–æ –¥–æ–±–∞–≤–ª—è—Ç—å"

# –°–æ–∑–¥–∞–Ω–∏–µ Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
with gr.Blocks(theme=gr.themes.Soft(), title="RAG Chat —Å VITS TTS") as demo:
    gr.Markdown("# üéØ RAG –ß–∞—Ç —Å VITS TTS")
    gr.Markdown("–û–±—â–∞–π—Ç–µ—Å—å —Å –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç –∏–ª–∏ –≥–æ–ª–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–∏–Ω—Ç–µ–∑–æ–º —Ä–µ—á–∏")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
            model_selector = gr.Dropdown(
                choices=available_models,
                value=default_model,
                label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å Ollama"
            )
            
            voice_input_checkbox = gr.Checkbox(
                label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥",
                value=False
            )
            
            audio_input = gr.Audio(
                sources=["microphone", "upload"],
                type="filepath",
                label="–ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥",
                visible=False
            )
            
            speech_speed = gr.Slider(
                minimum=0.5,
                maximum=2.0,
                value=1.0,
                step=0.1,
                label="–°–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏"
            )
            
            add_knowledge_btn = gr.UploadButton(
                "üìÅ –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π",
                file_types=[".txt"],
                file_count="multiple"
            )
            
            clear_btn = gr.Button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é", variant="secondary")
        
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(
                label="–î–∏–∞–ª–æ–≥",
                height=400,
                show_copy_button=True,
                avatar_images=(
                    "https://media.istockphoto.com/id/1334436084/vector/customer-service-icon-vector.jpg?s=612x612&w=0&k=20&c=K7VcXk-5nOZ25Y6Z8bqQ2pYVnRfqKfR6xQ9Xf6y1jqk=",
                    "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"
                )
            )
            
            recognized_text = gr.Textbox(
                label="–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
                interactive=True,  # –°–¥–µ–ª–∞–Ω–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–º
                visible=False
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    label="–¢–µ–∫—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥",
                    placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...",
                    lines=2,
                    scale=4,
                    interactive=True  # –°–¥–µ–ª–∞–Ω–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–º
                )
                send_btn = gr.Button("üì§ –û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary", scale=1)
            
            gr.Markdown("### –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏")
            with gr.Row():
                text_output = gr.Textbox(
                    label="–¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç",
                    interactive=True,  # –°–¥–µ–ª–∞–Ω–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–º
                    lines=3,
                    scale=4
                )
                add_to_kb_btn = gr.Button("üíæ –î–æ–±–∞–≤–∏—Ç—å –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π", scale=1)
            
            add_status = gr.Textbox(
                label="–°—Ç–∞—Ç—É—Å",
                interactive=False,
                visible=False
            )
            
            audio_output = gr.Audio(
                label="–ì–æ–ª–æ—Å–æ–≤–æ–π –æ—Ç–≤–µ—Ç (VITS)",
                interactive=False,
                type="filepath",
                autoplay=True
            )
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
    def toggle_voice_input(use_voice: bool):
        return (
            gr.Audio(visible=use_voice), 
            gr.Textbox(visible=not use_voice),
            gr.Textbox(visible=use_voice)
        )
    
    voice_input_checkbox.change(
        toggle_voice_input,
        inputs=[voice_input_checkbox],
        outputs=[audio_input, msg, recognized_text]
    )
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
    def send_message(message, audio, use_voice, speed):
        if not message and not audio:
            return "", "", "", None, "", ""
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞
        if use_voice and audio:
            recognized = voice_processor.speech_to_text(audio)
            if recognized.startswith("–û—à–∏–±–∫–∞") or recognized == "–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞":
                return "", "", recognized, None, recognized, ""
            input_text = recognized
        else:
            input_text = message
        
        if not input_text.strip():
            return "", "", "–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π", None, "", ""
        
        formatted_output, text_response, audio_response, recognized_display = chat_interface(
            input_text, audio, use_voice, speed
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —á–∞—Ç–±–æ—Ç–∞
        chat_history = [(input_text, text_response)]
        
        return "", chat_history, recognized_display, text_response, audio_response, ""
    
    send_btn.click(
        send_message,
        inputs=[msg, audio_input, voice_input_checkbox, speech_speed],
        outputs=[msg, chatbot, recognized_text, text_output, audio_output, add_status]
    )
    
    msg.submit(
        send_message,
        inputs=[msg, audio_input, voice_input_checkbox, speech_speed],
        outputs=[msg, chatbot, recognized_text, text_output, audio_output, add_status]
    )
    
    clear_btn.click(
        clear_chat,
        outputs=[chatbot, text_output, audio_output, recognized_text, add_status]
    )
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    add_to_kb_btn.click(
        add_text_to_knowledge,
        inputs=[text_output],
        outputs=[add_status]
    )
    
    def add_documents(files):
        if files:
            contents = []
            for file in files:
                try:
                    with open(file.name, 'r', encoding='utf-8') as f:
                        content = f.read()
                        contents.append(content)
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file.name}: {str(e)}"
            
            rag_system.add_to_knowledge_base(contents)
            rag_system.save_knowledge_base()
            return f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π. –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(rag_system.vector_db)}"
        return "–§–∞–π–ª—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã"
    
    add_knowledge_btn.upload(
        add_documents,
        inputs=[add_knowledge_btn],
        outputs=[add_status]
    )
    
    def update_model(model_name):
        rag_system.model_name = model_name
        rag_system.conversation_history.clear()
        return f"–ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model_name}"
    
    model_selector.change(
        update_model,
        inputs=[model_selector],
        outputs=[add_status]
    )

if __name__ == "__main__":    
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True
    )