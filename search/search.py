import gradio as gr
import feedparser
import requests
import subprocess
from duckduckgo_search import DDGS
from newspaper import Article  # –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
from bs4 import BeautifulSoup

# ---------- –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è ----------
titles_links_global = []

# ---------- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ----------
def search_duckduckgo(query, num=10):
    results = []
    with DDGS() as ddgs:
        for r in ddgs.news(query, max_results=num):
            results.append({
                "title": r.get("title"),
                "link": r.get("url"),
                "source": r.get("source"),
                "date": r.get("date")
            })
    return results

def search_newsapi(query, num=10, api_key="YOUR_API_KEY"):
    url = f"https://newsapi.org/v2/everything?q={query}&pageSize={num}&apiKey={api_key}"
    r = requests.get(url).json()
    results = []
    for article in r.get("articles", []):
        results.append({
            "title": article.get("title"),
            "link": article.get("url"),
            "source": article.get("source", {}).get("name"),
            "date": article.get("publishedAt")
        })
    return results

RSS_FEEDS = {
    "BBC": "http://feeds.bbci.co.uk/news/rss.xml",
    "Reuters": "http://feeds.reuters.com/reuters/topNews",
    "CNN": "http://rss.cnn.com/rss/edition.rss",
    "–£–ù–ò–ê–ù": "https://rss.unian.net/site/news_ukr.rss"
}

def search_rss(num=10):
    results = []
    for name, url in RSS_FEEDS.items():
        feed = feedparser.parse(url)
        for entry in feed.entries[:num//len(RSS_FEEDS)]:
            results.append({
                "title": entry.get("title"),
                "link": entry.get("link"),
                "source": name,
                "date": entry.get("published", "")
            })
    return results

def search_news(query, method):
    if method == "DuckDuckGo":
        return search_duckduckgo(query)
    elif method == "NewsAPI":
        return search_newsapi(query)
    elif method == "RSS":
        return search_rss()
    else:
        return [{"title": "–û—à–∏–±–∫–∞: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥", "link": "", "source": "", "date": ""}]

# ---------- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ----------
def format_results(results):
    global titles_links_global
    if not results:
        titles_links_global = []
        return "<b>–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.</b>", []
    html = "<ol>"
    titles_links_global = []
    news_titles = []
    for r in results:
        title = r['title'] or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        source = r['source'] or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"
        date = r['date'] or ""
        link = r['link'] or "#"
        html += f"<li><a href='{link}' target='_blank'>{title}</a> <i>({source}, {date})</i></li>"
        titles_links_global.append((title, link))
        news_titles.append(title)
    html += "</ol>"
    return html, news_titles

# ---------- –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Ollama ----------
def get_ollama_models():
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        models = []
        for line in result.stdout.splitlines()[1:]:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            parts = line.split()
            if parts:
                models.append(parts[0])
        return models if models else ["–ù–µ—Ç –º–æ–¥–µ–ª–µ–π"]
    except Exception as e:
        return [f"–û—à–∏–±–∫–∞: {e}"]

# ---------- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ ----------
def extract_article_text(url):
    # ---------- –ú–µ—Ç–æ–¥ 1: newspaper3k ----------
    try:
        article = Article(
            url,
            language='ru',
            browser_user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                "AppleWebKit/537.36 (KHTML, like Gecko) "
                                "Chrome/120.0.0.0 Safari/537.36"))
        article.download()
        article.parse()
        if article.text and len(article.text.split()) > 50:  # —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —Å—Ç–∞—Ç—å—è –∞–¥–µ–∫–≤–∞—Ç–Ω–∞ –µ—Å–ª–∏ >50 —Å–ª–æ–≤
            return article.text
    except Exception as e:
        pass  # –ø—Ä–æ—Å—Ç–æ –∏–¥—ë–º –¥–∞–ª—å—à–µ

    # ---------- –ú–µ—Ç–æ–¥ 2: requests + BeautifulSoup ----------
    try:
        headers = {"User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                                  "Chrome/120.0.0.0 Safari/537.36")}
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        paragraphs = [p.get_text() for p in soup.find_all("p")]
        text = "\n".join(paragraphs).strip()

        if text and len(text.split()) > 30:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –Ω–µ –ø—É—Å—Ç–∞—è
            return text
        else:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (—Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞)"
    except Exception as e:
        return f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏: {e}"


# ---------- –§—É–Ω–∫—Ü–∏—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ —á–µ—Ä–µ–∑ Ollama ----------
def summarize_news(news_text, prompt, llm_model):
    try:
        full_prompt = f"{prompt}\n\n{news_text}"
        result = subprocess.run(
                                ["ollama", "run", llm_model],
                                input=full_prompt,
                                text=True,
                                capture_output=True
                    )

        return result.stdout.strip() if result.stdout else "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {e}"

# ---------- UI ----------
with gr.Blocks() as demo:
    gr.Markdown("## üì∞ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π")

    with gr.Row():
        topic = gr.Textbox(label="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É", value="–í–æ–π–Ω–∞ –≤ –£–∫—Ä–∞–∏–Ω–µ")
        method = gr.Radio(["DuckDuckGo", "NewsAPI", "RSS"], value="DuckDuckGo", label="–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞")

    search_btn = gr.Button("–ü–æ–∏—Å–∫")
    output = gr.HTML(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    
    news_dropdown = gr.Dropdown(choices=[], label="–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è")
    llm_dropdown = gr.Dropdown(choices=get_ollama_models(), label="–í—ã–±–µ—Ä–∏—Ç–µ LLM (Ollama)", value=None)
    prompt_input = gr.Textbox(label="–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ", 
                              value="–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–æ–µ –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –≤–∏–¥–µ–æ YouTube.", lines=3)
    
    extract_btn = gr.Button("–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏")
    extract_output = gr.Textbox(label="–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏", lines=10)
    
    summarize_btn = gr.Button("–°–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—é–º–µ")
    summary_output = gr.Textbox(label="–†–µ–∑—é–º–µ", lines=10)
    
    # ---------- –õ–æ–≥–∏–∫–∞ ----------
    def search_and_update_dropdown(query, method, dropdown):
        html, news_titles = format_results(search_news(query, method))
        return html, gr.update(choices=news_titles, value=news_titles[0] if news_titles else None)

    search_btn.click(fn=search_and_update_dropdown, 
                     inputs=[topic, method, news_dropdown], 
                     outputs=[output, news_dropdown])

    def extract_selected(news_choice, prompt, llm_model):
        if not news_choice or not titles_links_global:
            return "–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –∏ –º–æ–¥–µ–ª—å"
        # –∏—â–µ–º —Å—Å—ã–ª–∫—É –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        link = None
        for title, lnk in titles_links_global:
            if title == news_choice:
                link = lnk
                break
        if not link:
            return "–°—Å—ã–ª–∫–∞ –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        news_text = extract_article_text(link)
        return news_text

    extract_btn.click(fn=extract_selected, 
                        inputs=[news_dropdown, prompt_input, llm_dropdown],
                        outputs=[extract_output])
    
    def summarize_text(news_text, prompt, llm_model):
        return summarize_news(news_text, prompt, llm_model)

    summarize_btn.click(fn=summarize_text, 
                        inputs=[extract_output, prompt_input, llm_dropdown],
                        outputs=[summary_output])    

demo.launch()